{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nnjw_IOcG7NAU7qrrIHQSm2MK5iq-CGz",
      "authorship_tag": "ABX9TyNp7rUbRFIe/lJvNG6sBSth",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JY522IC/PointCloudProject/blob/main/Segment_0603_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAqqSLDRKhhD"
      },
      "source": [
        "The original Pointnet work.\n",
        "Based on the paper:https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf\n",
        "Code:https://github.com/Young98CN/pointconv_pytorch\n",
        "Github: https://github.com/yanx27/Pointnet_Pointnet2_pytorch/blob/master/models/pointnet_utils.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the requred libraries"
      ],
      "metadata": {
        "id": "GLXWLYcyETDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4m6uApWEaVe",
        "outputId": "927bb9fb-6d0f-4b33-89f9-4da2d5db3eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYJvhDV33pzh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4016f7ec-eb9a-4a7c-b952-ef2cb044fd0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting open3d\n",
            "  Downloading open3d-0.17.0-cp310-cp310-manylinux_2_27_x86_64.whl (420.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.5/420.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.22.4)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-2.10.2-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.3.0)\n",
            "Collecting nbformat==5.7.0 (from open3d)\n",
            "  Downloading nbformat-5.7.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting configargparse (from open3d)\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting pillow>=9.3.0 (from open3d)\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.5.3)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.65.0)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat==5.7.0->open3d) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat==5.7.0->open3d) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat==5.7.0->open3d) (5.3.0)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat==5.7.0->open3d) (5.7.1)\n",
            "Requirement already satisfied: Flask<2.3.0,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.2.4)\n",
            "Collecting werkzeug>=2.2.3 (from open3d)\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.13.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.5.6)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Collecting widgetsnbextension~=4.0.7 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyterlab-widgets~=3.0.7 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (2.1.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<2.3.0,>=1.0.4->dash>=2.6.0->open3d) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<2.3.0,>=1.0.4->dash>=2.6.0->open3d) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<2.3.0,>=1.0.4->dash>=2.6.0->open3d) (8.1.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.4->open3d) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.4->open3d) (6.3.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.8.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat==5.7.0->open3d) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat==5.7.0->open3d) (0.19.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (8.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.16.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat==5.7.0->open3d) (3.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.6)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=8.0.4->open3d) (23.2.1)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, addict, widgetsnbextension, werkzeug, pyquaternion, pillow, jedi, configargparse, nbformat, dash, ipywidgets, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.4\n",
            "    Uninstalling widgetsnbextension-3.6.4:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.4\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 2.3.0\n",
            "    Uninstalling Werkzeug-2.3.0:\n",
            "      Successfully uninstalled Werkzeug-2.3.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: nbformat\n",
            "    Found existing installation: nbformat 5.8.0\n",
            "    Uninstalling nbformat-5.8.0:\n",
            "      Successfully uninstalled nbformat-5.8.0\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 configargparse-1.5.3 dash-2.10.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 ipywidgets-8.0.6 jedi-0.18.2 nbformat-5.7.0 open3d-0.17.0 pillow-9.5.0 pyquaternion-0.9.9 werkzeug-2.2.3 widgetsnbextension-4.0.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting provider\n",
            "  Downloading provider-0.11.0-py3-none-any.whl (33 kB)\n",
            "Collecting boto3<2.0.0,>=1.26.82 (from provider)\n",
            "  Downloading boto3-1.26.147-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from provider) (8.1.3)\n",
            "Collecting common-fate-schema<0.8.0,>=0.7.0 (from provider)\n",
            "  Downloading common_fate_schema-0.7.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.10.5 in /usr/local/lib/python3.10/dist-packages (from provider) (1.10.7)\n",
            "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.10/dist-packages (from provider) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from provider) (4.5.0)\n",
            "Collecting botocore<1.30.0,>=1.29.147 (from boto3<2.0.0,>=1.26.82->provider)\n",
            "  Downloading botocore-1.29.147-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.26.82->provider)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3<2.0.0,>=1.26.82->provider)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.147->boto3<2.0.0,>=1.26.82->provider) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.147->boto3<2.0.0,>=1.26.82->provider) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.147->boto3<2.0.0,>=1.26.82->provider) (1.16.0)\n",
            "Installing collected packages: jmespath, common-fate-schema, botocore, s3transfer, boto3, provider\n",
            "Successfully installed boto3-1.26.147 botocore-1.29.147 common-fate-schema-0.7.0 jmespath-1.0.1 provider-0.11.0 s3transfer-0.6.1\n",
            "2.0.1+cu118\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install the required libraries\n",
        "!pip install open3d\n",
        "!pip install provider\n",
        "\n",
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "# Helper functions for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def visualize_mesh(pos, face):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "    # ax = fig.gca(projection='3d')\n",
        "    ax.axes.xaxis.set_ticklabels([])\n",
        "    ax.axes.yaxis.set_ticklabels([])\n",
        "    ax.axes.zaxis.set_ticklabels([])\n",
        "    ax.plot_trisurf(pos[:, 0], pos[:, 1], pos[:, 2], triangles=data.face.t(), antialiased=False)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_points(pos, edge_index=None, index=None):\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    if edge_index is not None:\n",
        "        for (src, dst) in edge_index.t().tolist():\n",
        "             src = pos[src].tolist()\n",
        "             dst = pos[dst].tolist()\n",
        "             plt.plot([src[0], dst[0]], [src[1], dst[1]], linewidth=1, color='black')\n",
        "    if index is None:\n",
        "        plt.scatter(pos[:, 0], pos[:, 1], s=50, zorder=1000)\n",
        "    else:\n",
        "       mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
        "       mask[index] = True\n",
        "       plt.scatter(pos[~mask, 0], pos[~mask, 1], s=50, color='lightgray', zorder=1000)\n",
        "       plt.scatter(pos[mask, 0], pos[mask, 1], s=50, zorder=1000)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make the custom dataset"
      ],
      "metadata": {
        "id": "-vn-NdutbJnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch_geometric.data import InMemoryDataset, Data, download_url, extract_zip\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "c9rq-qKrbJFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inCube(X, corners):\n",
        "  \"\"\"\n",
        "  Check if a point `X` is inside of rectangular prison with `corners` (two points)\n",
        "  \"\"\"\n",
        "  # Where is X > corners?\n",
        "  greater = X > corners\n",
        "  # If X is greater than both corners of any axis, it is outside\n",
        "  inside = ~np.any(np.equal(*greater))\n",
        "\n",
        "  return inside"
      ],
      "metadata": {
        "id": "AmrfCkTCDZFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pc_normalize(pc):\n",
        "    \"\"\"\n",
        "    Point data normilization\n",
        "    :param pc: the pc to normilized\n",
        "    :return: the point data after normilization\n",
        "    \"\"\"\n",
        "    # the average value\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n",
        "    # rescale\n",
        "    pc = pc / m\n",
        "    return pc"
      ],
      "metadata": {
        "id": "ilBhua9jX4wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Operating_room_segment(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super(Operating_room_segment, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        # List of the raw files\n",
        "        return []\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        # return list of files should be in processed dir, if found - skip processing\n",
        "        return ['data.pt']\n",
        "\n",
        "    # def download(self):\n",
        "    #     # Download the file specified in self.url and store\n",
        "    #     # it in self.raw_dir\n",
        "    #     path = download_url(self.url, self.raw_dir)\n",
        "    #     extract_zip(path, self.raw_dir)\n",
        "    #     # The zip file is removed\n",
        "    #     os.unlink(path)\n",
        "\n",
        "\n",
        "    def process(self):\n",
        "\n",
        "      # In the loop we extract the nodes' embeddings, edges connectivity for\n",
        "      # and label for a graph, process the information and put it in a Data\n",
        "      # object, then we add the object to a list\n",
        "      data_list = []\n",
        "      root_new = '/content/drive/MyDrive/Colab Notebooks/Segmentation0529/Data0529/237115_import-pointclouds-ply_001_episodes_01 2/import-pointclouds-ply'\n",
        "\n",
        "      # load the annotations\n",
        "      annotation_root = os.path.join(root_new, 'annotation.json')\n",
        "      with open(annotation_root, 'r') as fcc_file:\n",
        "        annotation = json.load(fcc_file)\n",
        "\n",
        "      # load the annotations\n",
        "      frame_pointcloud_map_root = os.path.join(root_new, 'frame_pointcloud_map.json')\n",
        "      with open(frame_pointcloud_map_root, 'r') as fcc_file:\n",
        "        frame_pointcloud_map = json.load(fcc_file)\n",
        "\n",
        "      # Define the direction for the point cloud saved\n",
        "      pointcloud_data_root = os.path.join(root_new, 'pointcloud')\n",
        "\n",
        "      # Define the device and dtype to create new point cloud data\n",
        "      device = o3d.core.Device(\"CPU:0\")\n",
        "      dtype = o3d.core.float64\n",
        "\n",
        "      # Define the color and class map\n",
        "      Classes = ['Wall','Surgeon sitting on chair', 'Surgeon Standing',\n",
        "      'Table1','Table2','Table3','Table4']\n",
        "      # Classes = ['Wall','Surgeon sitting on chair', 'Surgeon Standing','Chair',\n",
        "      # 'Table1','Table2','Table3','Table4']\n",
        "      Color_class_map = {'Surgeon sitting on chair': [255, 0, 0], 'Surgeon Standing': [0, 255, 0], 'Chair': [0, 0, 255],\n",
        "                        'Table1': [255, 0, 255], 'Table2': [0, 255, 255], 'Table3': [221, 160, 221], 'Table4': [0, 255, 127]}\n",
        "\n",
        "      Class_title_map = {}\n",
        "      for i in annotation['objects']:\n",
        "          Class_title_map[i['key']] = i['classTitle']\n",
        "\n",
        "      # Wall and floor is class 0\n",
        "      # Class_label_map = {'Surgeon sitting on chair': 1,\n",
        "      #           'Surgeon Standing': 2,\n",
        "      #           'Chair': 3,\n",
        "      #           'Table1': 4,\n",
        "      #           'Table2': 5,\n",
        "      #           'Table3': 6,\n",
        "      #           'Table4': 7}\n",
        "      Class_label_map = {'Surgeon sitting on chair': 1,\n",
        "          'Surgeon Standing': 2,\n",
        "          'Table1': 3,\n",
        "          'Table2': 4,\n",
        "          'Table3': 5,\n",
        "          'Table4': 6}\n",
        "\n",
        "      # Do the point cloud data process\n",
        "      number_of_files = annotation['framesCount']\n",
        "\n",
        "      # Define the labelweights\n",
        "      labelweights = np.zeros(len(Classes))\n",
        "\n",
        "      for i in range(number_of_files):\n",
        "\n",
        "        # Define the file path\n",
        "        file_index = annotation['frames'][i]['index']\n",
        "        file_name = frame_pointcloud_map[str(file_index)]\n",
        "        file_dir = os.path.join(root_new,'pointcloud', file_name)\n",
        "\n",
        "        # Load the pcd file\n",
        "        pcd = o3d.io.read_point_cloud(file_dir)\n",
        "\n",
        "        # Control the number of points in the point cloud file\n",
        "        pcd = pcd.farthest_point_down_sample(15000)\n",
        "\n",
        "        point_pos = np.asarray(pcd.points)\n",
        "        point_color = np.asarray(pcd.colors)\n",
        "\n",
        "        # Define the ground truth colors matrix\n",
        "        colors_ground_truth_matrix = np.full(np.shape(point_pos), fill_value=192)\n",
        "        Y_labels = np.zeros(np.shape(point_pos)[0])\n",
        "\n",
        "        # Labels\n",
        "        for j in range(np.shape(point_pos)[0]):\n",
        "\n",
        "            # Get points position\n",
        "            X = np.array(point_pos[j])\n",
        "\n",
        "            for k in annotation['frames'][i]['figures']:\n",
        "                x_min = k['geometry']['position']['x'] - k['geometry']['dimensions']['x'] / 2\n",
        "                x_max = k['geometry']['position']['x'] + k['geometry']['dimensions']['x'] / 2\n",
        "\n",
        "                y_min = k['geometry']['position']['y'] - k['geometry']['dimensions']['y'] / 2\n",
        "                y_max = k['geometry']['position']['y'] + k['geometry']['dimensions']['y'] / 2\n",
        "\n",
        "                z_min = k['geometry']['position']['z'] - k['geometry']['dimensions']['z'] / 2\n",
        "                z_max = k['geometry']['position']['z'] + k['geometry']['dimensions']['z'] / 2\n",
        "\n",
        "                corners = [[x_min, y_min, z_min], [x_max, y_max, z_max]]\n",
        "\n",
        "                if inCube(X, corners):\n",
        "                    colors_ground_truth_matrix[j] = Color_class_map[Class_title_map[k['objectKey']]]\n",
        "                    Y_labels[j] = Class_label_map[Class_title_map[k['objectKey']]]\n",
        "\n",
        "        # Use histogram to calculate the sample weight\n",
        "        tmp, _ = np.histogram(Y_labels, range(len(Classes)+1))\n",
        "        print('tmp')\n",
        "        print(tmp)\n",
        "        labelweights += tmp\n",
        "\n",
        "        # Normilize the data\n",
        "        new_points = pc_normalize(point_pos)\n",
        "\n",
        "        # Calculate the normal information\n",
        "        device = o3d.core.Device(\"CPU:0\")\n",
        "        dtype = o3d.core.float64\n",
        "        pcd_new = o3d.t.geometry.PointCloud(device)\n",
        "        pcd_new.point.positions = o3d.core.Tensor(new_points, dtype, device)\n",
        "        pcd_new = pcd_new.to_legacy()\n",
        "        radius = 0.01  # Searching Radius\n",
        "        max_nn = 30  # Number of neighboring points to estimate normal vector\n",
        "        pcd_new.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius, max_nn))\n",
        "\n",
        "        # Convert the position and label information into the Pytorch format\n",
        "        Pos = torch.tensor(new_points,dtype=torch.float32)\n",
        "        Y = torch.tensor(Y_labels,dtype=torch.long)\n",
        "        Normals = torch.tensor(np.asarray(pcd_new.normals),dtype=torch.float32)\n",
        "        File_name_index = torch.tensor(file_index,dtype=torch.long)\n",
        "        Colors = torch.tensor(point_color,dtype=torch.float32)\n",
        "        Colors_ground_truth = torch.tensor(colors_ground_truth_matrix,dtype=torch.float32)\n",
        "\n",
        "        # Save the data\n",
        "        graph = Data(pos = Pos, y = Y, normal = Normals, file_name_index = File_name_index, colors = Colors, colors_ground_truth = Colors_ground_truth)\n",
        "        data_list.append(graph)\n",
        "\n",
        "        # print(index)\n",
        "\n",
        "      # Calculate the sample weight can save in the same index as dataset direction\n",
        "      labelweights = labelweights.astype(np.float32)\n",
        "      labelweights = labelweights / np.sum(labelweights)\n",
        "      labelweights = np.power(np.amax(labelweights) / labelweights, 1 / 3.0)\n",
        "      print(labelweights)\n",
        "      weight_dirction = os.path.join(self.root,'SampleWeightArray.npy')\n",
        "      np.save(weight_dirction,labelweights)\n",
        "\n",
        "      # Apply the functions specified in pre_filter and pre_transform\n",
        "      if self.pre_filter is not None:\n",
        "          data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "      if self.pre_transform is not None:\n",
        "          data_list = [self.pre_transform(data) for data in data_list]\n",
        "\n",
        "      # Store the processed data\n",
        "      data, slices = self.collate(data_list)\n",
        "      torch.save((data, slices), self.processed_paths[0])"
      ],
      "metadata": {
        "id": "_N3iP2vtcvoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric.transforms as T\n",
        "# train_index = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/InterestAreaClassfication0521/Datasetmade/InterestArea0523/train0523.csv')\n",
        "# test_index = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/InterestAreaClassfication0521/Datasetmade/InterestArea0523/test0523.csv')\n",
        "\n",
        "# Create the train and test dataset based on the direction\n",
        "# State = 1\n",
        "trainDataset = Operating_room_segment(root='/content/drive/MyDrive/Colab Notebooks/Segmentation0529/Dataset/Dataset0603/Train')\n",
        "# State = 2\n",
        "testDataset = Operating_room_segment(root='/content/drive/MyDrive/Colab Notebooks/Segmentation0529/Dataset/Dataset0603/Test')"
      ],
      "metadata": {
        "id": "pyOOA7njdALx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network"
      ],
      "metadata": {
        "id": "cQSDUyttTJPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inplace_relu(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('ReLU') != -1:\n",
        "        m.inplace=True"
      ],
      "metadata": {
        "id": "PiV6enz6cBzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class STN3d(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(STN3d, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 9)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.size()[0]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        iden = Variable(torch.from_numpy(np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32))).view(1, 9).repeat(\n",
        "            batchsize, 1)\n",
        "        if x.is_cuda:\n",
        "            iden = iden.cuda()\n",
        "        x = x + iden\n",
        "        x = x.view(-1, 3, 3)\n",
        "        return x\n",
        "\n",
        "class STNkd(nn.Module):\n",
        "    def __init__(self, k=64):\n",
        "        super(STNkd, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, k * k)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.size()[0]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1, self.k * self.k).repeat(\n",
        "            batchsize, 1)\n",
        "        if x.is_cuda:\n",
        "            iden = iden.cuda()\n",
        "        x = x + iden\n",
        "        x = x.view(-1, self.k, self.k)\n",
        "        return x\n",
        "\n",
        "class PointNetEncoder(nn.Module):\n",
        "    def __init__(self, global_feat=True, feature_transform=False, channel=3):\n",
        "        super(PointNetEncoder, self).__init__()\n",
        "        self.stn = STN3d(channel)\n",
        "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.global_feat = global_feat\n",
        "        self.feature_transform = feature_transform\n",
        "        if self.feature_transform:\n",
        "            self.fstn = STNkd(k=64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, D, N = x.size()\n",
        "        trans = self.stn(x)\n",
        "        x = x.transpose(2, 1)\n",
        "        if D > 3:\n",
        "            feature = x[:, :, 3:]\n",
        "            x = x[:, :, :3]\n",
        "        x = torch.bmm(x, trans)\n",
        "        if D > 3:\n",
        "            x = torch.cat([x, feature], dim=2)\n",
        "        x = x.transpose(2, 1)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        if self.feature_transform:\n",
        "            trans_feat = self.fstn(x)\n",
        "            x = x.transpose(2, 1)\n",
        "            x = torch.bmm(x, trans_feat)\n",
        "            x = x.transpose(2, 1)\n",
        "        else:\n",
        "            trans_feat = None\n",
        "\n",
        "        pointfeat = x\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "        if self.global_feat:\n",
        "            return x, trans, trans_feat\n",
        "        else:\n",
        "            x = x.view(-1, 1024, 1).repeat(1, 1, N)\n",
        "            return torch.cat([x, pointfeat], 1), trans, trans_feat\n",
        "\n",
        "\n",
        "def feature_transform_reguliarzer(trans):\n",
        "    d = trans.size()[1]\n",
        "    I = torch.eye(d)[None, :, :]\n",
        "    if trans.is_cuda:\n",
        "        I = I.cuda()\n",
        "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2, 1)) - I, dim=(1, 2)))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "lJ6RClLjTOx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the main network structure"
      ],
      "metadata": {
        "id": "wZssaSYfcItk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class get_model(nn.Module):\n",
        "    def __init__(self, num_class):\n",
        "        super(get_model, self).__init__()\n",
        "        self.k = num_class\n",
        "        self.feat = PointNetEncoder(global_feat=False, feature_transform=True, channel=9)\n",
        "        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n",
        "        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.size()[0]\n",
        "        n_pts = x.size()[2]\n",
        "        x, trans, trans_feat = self.feat(x)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.conv4(x)\n",
        "        x = x.transpose(2,1).contiguous()\n",
        "        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n",
        "        x = x.view(batchsize, n_pts, self.k)\n",
        "        return x, trans_feat\n",
        "\n",
        "class get_loss(torch.nn.Module):\n",
        "    def __init__(self, mat_diff_loss_scale=0.001):\n",
        "        super(get_loss, self).__init__()\n",
        "        self.mat_diff_loss_scale = mat_diff_loss_scale\n",
        "\n",
        "    def forward(self, pred, target, trans_feat, weight):\n",
        "        loss = F.nll_loss(pred, target, weight = weight)\n",
        "        mat_diff_loss = feature_transform_reguliarzer(trans_feat)\n",
        "        total_loss = loss + mat_diff_loss * self.mat_diff_loss_scale\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "6454T34pcG6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Main train and test structure"
      ],
      "metadata": {
        "id": "Un6A5vG0d6TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_vote(vote_label_pool, point_idx, pred_label, weight):\n",
        "    B = pred_label.shape[0]\n",
        "    N = pred_label.shape[1]\n",
        "    for b in range(B):\n",
        "        for n in range(N):\n",
        "            if weight[b, n] != 0 and not np.isinf(weight[b, n]):\n",
        "                vote_label_pool[int(point_idx[b, n]), int(pred_label[b, n])] += 1\n",
        "    return vote_label_pool"
      ],
      "metadata": {
        "id": "xau79g9KLvWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import provider\n",
        "import importlib\n",
        "import shutil\n",
        "import argparse\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "# Define the class and label maps\n",
        "classes = ['Wall','Surgeon sitting on chair', 'Surgeon Standing',\n",
        "      'Table1','Table2','Table3','Table4']\n",
        "class2label = {cls: i for i, cls in enumerate(classes)}\n",
        "seg_classes = class2label\n",
        "seg_label_to_cat = {}\n",
        "\n",
        "for i, cat in enumerate(seg_classes.keys()):\n",
        "    seg_label_to_cat[i] = cat\n",
        "\n",
        "# Define the required parameters\n",
        "num_class = len(classes)\n",
        "# Number of classes in the dataset\n",
        "log_dir = None\n",
        "# Define the log dir\n",
        "npoint = 15000\n",
        "# Point number\n",
        "viusal = True\n",
        "# visualize result [default: False]\n",
        "num_votes = 5\n",
        "# aggregate segmentation scores with voting [default: 5]\n",
        "\n",
        "# Define the log\n",
        "def log_string(str):\n",
        "  logger.info(str)\n",
        "  print(str)\n",
        "\n",
        "'''HYPER PARAMETER'''\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "experiment_dir = '/content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31'\n",
        "visual_dir = experiment_dir + '/visual/'\n",
        "visual_dir = Path(visual_dir)\n",
        "visual_dir.mkdir(exist_ok=True)\n",
        "\n",
        "'''LOG'''\n",
        "logger = logging.getLogger(\"Model\")\n",
        "logger.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "file_handler = logging.FileHandler('%s/eval.txt' % experiment_dir)\n",
        "file_handler.setLevel(logging.INFO)\n",
        "file_handler.setFormatter(formatter)\n",
        "logger.addHandler(file_handler)\n",
        "log_string('PARAMETER ...')\n",
        "\n",
        "# Load the data\n",
        "testDataLoader = DataLoader(testDataset, batch_size=3, shuffle=True)\n",
        "weights = np.load(\"/content/drive/MyDrive/Colab Notebooks/Segmentation0529/Dataset/Dataset0603/Test/SampleWeightArray.npy\")\n",
        "weights = torch.Tensor(weights)\n",
        "log_string(\"The number of test data is: %d\" % len(testDataset))\n",
        "\n",
        "# Load the model\n",
        "classifier = get_model(num_class)\n",
        "checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n",
        "classifier.load_state_dict(checkpoint['model_state_dict'])\n",
        "classifier = classifier.eval()\n",
        "\n",
        "\n",
        "num_batches = len(testDataLoader)\n",
        "total_correct = 0\n",
        "total_seen = 0\n",
        "loss_sum = 0\n",
        "labelweights = np.zeros(num_class)\n",
        "total_seen_class_tmp = [0 for _ in range(num_class)]\n",
        "total_correct_class_tmp = [0 for _ in range(num_class)]\n",
        "total_iou_deno_class_tmp = [0 for _ in range(num_class)]\n",
        "if args.visual:\n",
        "  fout = open(os.path.join(visual_dir, scene_id[batch_idx] + '_pred.obj'), 'w')\n",
        "  fout_gt = open(os.path.join(visual_dir, scene_id[batch_idx] + '_gt.obj'), 'w')\n",
        "\n",
        "\n",
        "for data in tqdm(testDataLoader, desc='Processing'):\n",
        "  points = np.asarray(data.pos)\n",
        "  color = np.asarray(data.colors)\n",
        "  normal = np.asarray(data.normal)\n",
        "  points = np.reshape(points,[int(np.shape(points)[0]/npoint),npoint,3])\n",
        "  color = np.reshape(color,[int(np.shape(normal)[0]/npoint),npoint,3])\n",
        "  normal = np.reshape(normal,[int(np.shape(normal)[0]/npoint),npoint,3])\n",
        "  new_points = np.concatenate((points,color,normal),axis=2)\n",
        "  points = torch.Tensor(new_points)\n",
        "  points = points.transpose(2, 1)\n",
        "\n",
        "  seg_pred, trans_feat = classifier(points)\n",
        "  pred_val = seg_pred.contiguous().cpu().data.numpy()\n",
        "  seg_pred = seg_pred.contiguous().view(-1, num_class)\n",
        "\n",
        "  batch_label = np.asarray(data.y)\n",
        "  batch_label = np.reshape(batch_label,[np.shape(points)[0],npoint])\n",
        "  target = data.y\n",
        "\n",
        "  pred_val = np.argmax(pred_val, 2)\n",
        "  correct = np.sum((pred_val == batch_label))\n",
        "  total_correct += correct\n",
        "  total_seen += (np.shape(points)[0] * npoint)\n",
        "  tmp, _ = np.histogram(batch_label, range(num_class + 1))\n",
        "  labelweights += tmp\n",
        "\n",
        "  for l in range(num_class):\n",
        "    total_seen_class_tmp[l] += np.sum((whole_scene_label == l))\n",
        "    total_correct_class_tmp[l] += np.sum((pred_label == l) & (whole_scene_label == l))\n",
        "    total_iou_deno_class_tmp[l] += np.sum(((pred_label == l) | (whole_scene_label == l)))\n",
        "    total_seen_class[l] += total_seen_class_tmp[l]\n",
        "    total_correct_class[l] += total_correct_class_tmp[l]\n",
        "    total_iou_deno_class[l] += total_iou_deno_class_tmp[l]\n",
        "\n",
        "  iou_map = np.array(total_correct_class_tmp) / (np.array(total_iou_deno_class_tmp, dtype=np.float) + 1e-6)\n",
        "  print(iou_map)\n",
        "  arr = np.array(total_seen_class_tmp)\n",
        "  tmp_iou = np.mean(iou_map[arr != 0])\n",
        "  log_string('Mean IoU of %s: %.4f' % (scene_id[batch_idx], tmp_iou))\n",
        "  print('----------------------------')\n",
        "\n",
        "  filename = os.path.join(visual_dir, scene_id[batch_idx] + '.txt')\n",
        "  with open(filename, 'w') as pl_save:\n",
        "      for i in pred_label:\n",
        "          pl_save.write(str(int(i)) + '\\n')\n",
        "      pl_save.close()\n",
        "  for i in range(whole_scene_label.shape[0]):\n",
        "      color = g_label2color[pred_label[i]]\n",
        "      color_gt = g_label2color[whole_scene_label[i]]\n",
        "      if args.visual:\n",
        "          fout.write('v %f %f %f %d %d %d\\n' % (\n",
        "              whole_scene_data[i, 0], whole_scene_data[i, 1], whole_scene_data[i, 2], color[0], color[1],\n",
        "              color[2]))\n",
        "          fout_gt.write(\n",
        "              'v %f %f %f %d %d %d\\n' % (\n",
        "                  whole_scene_data[i, 0], whole_scene_data[i, 1], whole_scene_data[i, 2], color_gt[0],\n",
        "                  color_gt[1], color_gt[2]))\n",
        "  if args.visual:\n",
        "      fout.close()\n",
        "      fout_gt.close()\n",
        "\n",
        "IoU = np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=np.float) + 1e-6)\n",
        "iou_per_class_str = '------- IoU --------\\n'\n",
        "for l in range(NUM_CLASSES):\n",
        "  iou_per_class_str += 'class %s, IoU: %.3f \\n' % (\n",
        "      seg_label_to_cat[l] + ' ' * (14 - len(seg_label_to_cat[l])),\n",
        "      total_correct_class[l] / float(total_iou_deno_class[l]))\n",
        "log_string(iou_per_class_str)\n",
        "log_string('eval point avg class IoU: %f' % np.mean(IoU))\n",
        "log_string('eval whole scene point avg class acc: %f' % (\n",
        "  np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=np.float) + 1e-6))))\n",
        "log_string('eval whole scene point accuracy: %f' % (\n",
        "      np.sum(total_correct_class) / float(np.sum(total_seen_class) + 1e-6)))\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "cckIjC3id-J2",
        "outputId": "cf60d9ba-c491-4c0c-f826-51b114705ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Model:PARAMETER ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PARAMETER ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Model:The number of training data is: 3\n",
            "INFO:Model:The number of test data is: 3\n",
            "INFO:Model:No existing model, starting training from scratch...\n",
            "INFO:Model:Start training...\n",
            "INFO:Model:Epoch 1 (1/0):\n",
            "INFO:Model:Learning rate:0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of training data is: 3\n",
            "The number of test data is: 3\n",
            "No existing model, starting training from scratch...\n",
            "Epoch 1 (1/0):\n",
            "Learning rate:0.000100\n",
            "BN momentum updated to: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1/1 [00:11<00:00, 11.20s/it]\n",
            "INFO:Model:Training mean loss: 0.728121\n",
            "INFO:Model:Training accuracy: 0.088244\n",
            "INFO:Model:Save model...\n",
            "INFO:Model:Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/model.pth\n",
            "INFO:Model:Saving model....\n",
            "INFO:Model:---- EPOCH 001 EVALUATION ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mean loss: 0.728121\n",
            "Training accuracy: 0.088244\n",
            "Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/model.pth\n",
            "Saving model....\n",
            "---- EPOCH 001 EVALUATION ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1/1 [00:03<00:00,  3.77s/it]\n",
            "<ipython-input-11-f04e38c89913>:302: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mIoU = np.mean(np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=np.float) + 1e-6))\n",
            "INFO:Model:eval mean loss: 1.969143\n",
            "INFO:Model:eval point avg class IoU: 0.003908\n",
            "INFO:Model:eval point accuracy: 0.027356\n",
            "<ipython-input-11-f04e38c89913>:307: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=np.float) + 1e-6))))\n",
            "INFO:Model:eval point avg class acc: 0.142857\n",
            "INFO:Model:------- IoU --------\n",
            "class Wall     weight: 0.054, IoU: 0.000 \n",
            "class Surgeon sitting on chair weight: 0.781, IoU: 0.000 \n",
            "class Surgeon Standing weight: 0.031, IoU: 0.000 \n",
            "class Table1   weight: 0.041, IoU: 0.000 \n",
            "class Table2   weight: 0.043, IoU: 0.000 \n",
            "class Table3   weight: 0.023, IoU: 0.027 \n",
            "class Table4   weight: 0.027, IoU: 0.000 \n",
            "\n",
            "INFO:Model:Eval mean loss: 1.969143\n",
            "INFO:Model:Eval accuracy: 0.027356\n",
            "INFO:Model:Save model...\n",
            "INFO:Model:Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/best_model.pth\n",
            "INFO:Model:Saving model....\n",
            "INFO:Model:Best mIoU: 0.003908\n",
            "INFO:Model:Epoch 2 (2/1):\n",
            "INFO:Model:Learning rate:0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval mean loss: 1.969143\n",
            "eval point avg class IoU: 0.003908\n",
            "eval point accuracy: 0.027356\n",
            "eval point avg class acc: 0.142857\n",
            "------- IoU --------\n",
            "class Wall     weight: 0.054, IoU: 0.000 \n",
            "class Surgeon sitting on chair weight: 0.781, IoU: 0.000 \n",
            "class Surgeon Standing weight: 0.031, IoU: 0.000 \n",
            "class Table1   weight: 0.041, IoU: 0.000 \n",
            "class Table2   weight: 0.043, IoU: 0.000 \n",
            "class Table3   weight: 0.023, IoU: 0.027 \n",
            "class Table4   weight: 0.027, IoU: 0.000 \n",
            "\n",
            "Eval mean loss: 1.969143\n",
            "Eval accuracy: 0.027356\n",
            "Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/best_model.pth\n",
            "Saving model....\n",
            "Best mIoU: 0.003908\n",
            "Epoch 2 (2/1):\n",
            "Learning rate:0.000100\n",
            "BN momentum updated to: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1/1 [00:11<00:00, 11.26s/it]\n",
            "INFO:Model:Training mean loss: 0.728288\n",
            "INFO:Model:Training accuracy: 0.063978\n",
            "INFO:Model:---- EPOCH 002 EVALUATION ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mean loss: 0.728288\n",
            "Training accuracy: 0.063978\n",
            "---- EPOCH 002 EVALUATION ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it]\n",
            "INFO:Model:eval mean loss: 1.979540\n",
            "INFO:Model:eval point avg class IoU: 0.003908\n",
            "INFO:Model:eval point accuracy: 0.027356\n",
            "INFO:Model:eval point avg class acc: 0.142857\n",
            "INFO:Model:------- IoU --------\n",
            "class Wall     weight: 0.054, IoU: 0.000 \n",
            "class Surgeon sitting on chair weight: 0.781, IoU: 0.000 \n",
            "class Surgeon Standing weight: 0.031, IoU: 0.000 \n",
            "class Table1   weight: 0.041, IoU: 0.000 \n",
            "class Table2   weight: 0.043, IoU: 0.000 \n",
            "class Table3   weight: 0.023, IoU: 0.027 \n",
            "class Table4   weight: 0.027, IoU: 0.000 \n",
            "\n",
            "INFO:Model:Eval mean loss: 1.979540\n",
            "INFO:Model:Eval accuracy: 0.027356\n",
            "INFO:Model:Save model...\n",
            "INFO:Model:Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval mean loss: 1.979540\n",
            "eval point avg class IoU: 0.003908\n",
            "eval point accuracy: 0.027356\n",
            "eval point avg class acc: 0.142857\n",
            "------- IoU --------\n",
            "class Wall     weight: 0.054, IoU: 0.000 \n",
            "class Surgeon sitting on chair weight: 0.781, IoU: 0.000 \n",
            "class Surgeon Standing weight: 0.031, IoU: 0.000 \n",
            "class Table1   weight: 0.041, IoU: 0.000 \n",
            "class Table2   weight: 0.043, IoU: 0.000 \n",
            "class Table3   weight: 0.023, IoU: 0.027 \n",
            "class Table4   weight: 0.027, IoU: 0.000 \n",
            "\n",
            "Eval mean loss: 1.979540\n",
            "Eval accuracy: 0.027356\n",
            "Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Model:Saving model....\n",
            "INFO:Model:Best mIoU: 0.003908\n",
            "INFO:Model:Epoch 3 (3/2):\n",
            "INFO:Model:Learning rate:0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model....\n",
            "Best mIoU: 0.003908\n",
            "Epoch 3 (3/2):\n",
            "Learning rate:0.000100\n",
            "BN momentum updated to: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1/1 [00:11<00:00, 11.12s/it]\n",
            "INFO:Model:Training mean loss: 0.718403\n",
            "INFO:Model:Training accuracy: 0.065644\n",
            "INFO:Model:---- EPOCH 003 EVALUATION ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mean loss: 0.718403\n",
            "Training accuracy: 0.065644\n",
            "---- EPOCH 003 EVALUATION ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1/1 [00:03<00:00,  3.09s/it]\n",
            "INFO:Model:eval mean loss: 1.991815\n",
            "INFO:Model:eval point avg class IoU: 0.003908\n",
            "INFO:Model:eval point accuracy: 0.027356\n",
            "INFO:Model:eval point avg class acc: 0.142857\n",
            "INFO:Model:------- IoU --------\n",
            "class Wall     weight: 0.054, IoU: 0.000 \n",
            "class Surgeon sitting on chair weight: 0.781, IoU: 0.000 \n",
            "class Surgeon Standing weight: 0.031, IoU: 0.000 \n",
            "class Table1   weight: 0.041, IoU: 0.000 \n",
            "class Table2   weight: 0.043, IoU: 0.000 \n",
            "class Table3   weight: 0.023, IoU: 0.027 \n",
            "class Table4   weight: 0.027, IoU: 0.000 \n",
            "\n",
            "INFO:Model:Eval mean loss: 1.991815\n",
            "INFO:Model:Eval accuracy: 0.027356\n",
            "INFO:Model:Save model...\n",
            "INFO:Model:Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/best_model.pth\n",
            "INFO:Model:Saving model....\n",
            "INFO:Model:Best mIoU: 0.003908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval mean loss: 1.991815\n",
            "eval point avg class IoU: 0.003908\n",
            "eval point accuracy: 0.027356\n",
            "eval point avg class acc: 0.142857\n",
            "------- IoU --------\n",
            "class Wall     weight: 0.054, IoU: 0.000 \n",
            "class Surgeon sitting on chair weight: 0.781, IoU: 0.000 \n",
            "class Surgeon Standing weight: 0.031, IoU: 0.000 \n",
            "class Table1   weight: 0.041, IoU: 0.000 \n",
            "class Table2   weight: 0.043, IoU: 0.000 \n",
            "class Table3   weight: 0.023, IoU: 0.027 \n",
            "class Table4   weight: 0.027, IoU: 0.000 \n",
            "\n",
            "Eval mean loss: 1.991815\n",
            "Eval accuracy: 0.027356\n",
            "Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/best_model.pth\n",
            "Saving model....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Model:Epoch 4 (4/3):\n",
            "INFO:Model:Learning rate:0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best mIoU: 0.003908\n",
            "Epoch 4 (4/3):\n",
            "Learning rate:0.000100\n",
            "BN momentum updated to: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1/1 [00:10<00:00, 10.00s/it]\n",
            "INFO:Model:Training mean loss: 0.717776\n",
            "INFO:Model:Training accuracy: 0.155622\n",
            "INFO:Model:---- EPOCH 004 EVALUATION ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mean loss: 0.717776\n",
            "Training accuracy: 0.155622\n",
            "---- EPOCH 004 EVALUATION ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1/1 [00:03<00:00,  3.94s/it]\n",
            "INFO:Model:eval mean loss: 2.010611\n",
            "INFO:Model:eval point avg class IoU: 0.003920\n",
            "INFO:Model:eval point accuracy: 0.027422\n",
            "INFO:Model:eval point avg class acc: 0.142869\n",
            "INFO:Model:------- IoU --------\n",
            "class Wall     weight: 0.054, IoU: 0.000 \n",
            "class Surgeon sitting on chair weight: 0.781, IoU: 0.000 \n",
            "class Surgeon Standing weight: 0.031, IoU: 0.000 \n",
            "class Table1   weight: 0.041, IoU: 0.000 \n",
            "class Table2   weight: 0.043, IoU: 0.000 \n",
            "class Table3   weight: 0.023, IoU: 0.027 \n",
            "class Table4   weight: 0.027, IoU: 0.000 \n",
            "\n",
            "INFO:Model:Eval mean loss: 2.010611\n",
            "INFO:Model:Eval accuracy: 0.027422\n",
            "INFO:Model:Save model...\n",
            "INFO:Model:Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval mean loss: 2.010611\n",
            "eval point avg class IoU: 0.003920\n",
            "eval point accuracy: 0.027422\n",
            "eval point avg class acc: 0.142869\n",
            "------- IoU --------\n",
            "class Wall     weight: 0.054, IoU: 0.000 \n",
            "class Surgeon sitting on chair weight: 0.781, IoU: 0.000 \n",
            "class Surgeon Standing weight: 0.031, IoU: 0.000 \n",
            "class Table1   weight: 0.041, IoU: 0.000 \n",
            "class Table2   weight: 0.043, IoU: 0.000 \n",
            "class Table3   weight: 0.023, IoU: 0.027 \n",
            "class Table4   weight: 0.027, IoU: 0.000 \n",
            "\n",
            "Eval mean loss: 2.010611\n",
            "Eval accuracy: 0.027422\n",
            "Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:Model:Saving model....\n",
            "INFO:Model:Best mIoU: 0.003920\n",
            "INFO:Model:Epoch 5 (5/4):\n",
            "INFO:Model:Learning rate:0.000100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model....\n",
            "Best mIoU: 0.003920\n",
            "Epoch 5 (5/4):\n",
            "Learning rate:0.000100\n",
            "BN momentum updated to: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1/1 [00:14<00:00, 14.23s/it]\n",
            "INFO:Model:Training mean loss: 0.709024\n",
            "INFO:Model:Training accuracy: 0.082756\n",
            "INFO:Model:---- EPOCH 005 EVALUATION ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mean loss: 0.709024\n",
            "Training accuracy: 0.082756\n",
            "---- EPOCH 005 EVALUATION ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 1/1 [00:05<00:00,  5.58s/it]\n",
            "INFO:Model:eval mean loss: 2.025725\n",
            "INFO:Model:eval point avg class IoU: 0.020593\n",
            "INFO:Model:eval point accuracy: 0.116333\n",
            "INFO:Model:eval point avg class acc: 0.154550\n",
            "INFO:Model:------- IoU --------\n",
            "class Wall     weight: 0.054, IoU: 0.115 \n",
            "class Surgeon sitting on chair weight: 0.781, IoU: 0.000 \n",
            "class Surgeon Standing weight: 0.031, IoU: 0.000 \n",
            "class Table1   weight: 0.041, IoU: 0.000 \n",
            "class Table2   weight: 0.043, IoU: 0.000 \n",
            "class Table3   weight: 0.023, IoU: 0.029 \n",
            "class Table4   weight: 0.027, IoU: 0.000 \n",
            "\n",
            "INFO:Model:Eval mean loss: 2.025725\n",
            "INFO:Model:Eval accuracy: 0.116333\n",
            "INFO:Model:Save model...\n",
            "INFO:Model:Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/best_model.pth\n",
            "INFO:Model:Saving model....\n",
            "INFO:Model:Best mIoU: 0.020593\n",
            "INFO:Model:End of training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval mean loss: 2.025725\n",
            "eval point avg class IoU: 0.020593\n",
            "eval point accuracy: 0.116333\n",
            "eval point avg class acc: 0.154550\n",
            "------- IoU --------\n",
            "class Wall     weight: 0.054, IoU: 0.115 \n",
            "class Surgeon sitting on chair weight: 0.781, IoU: 0.000 \n",
            "class Surgeon Standing weight: 0.031, IoU: 0.000 \n",
            "class Table1   weight: 0.041, IoU: 0.000 \n",
            "class Table2   weight: 0.043, IoU: 0.000 \n",
            "class Table3   weight: 0.023, IoU: 0.029 \n",
            "class Table4   weight: 0.027, IoU: 0.000 \n",
            "\n",
            "Eval mean loss: 2.025725\n",
            "Eval accuracy: 0.116333\n",
            "Saving at /content/drive/MyDrive/Colab Notebooks/Segmentation0529/Result/Segment0603try/sem_seg/2023-06-03_21-31/checkpoints/best_model.pth\n",
            "Saving model....\n",
            "Best mIoU: 0.020593\n"
          ]
        }
      ]
    }
  ]
}